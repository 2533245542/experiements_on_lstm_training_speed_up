## LSTM training speed
### Introduction
We examine two factors: batch size and GPU/CPU.

### Procedures
Modify the TODO's in `examine_factors_that_impact_lstm_training_speed.py` to run the experiments.

`python examine_factors_that_impact_lstm_training_speed.py`

### Results
Batch size has a significant impact: batch size changes from 1 to 160 -> speed up for 40 fold.

GPU/CPU does not have a significant impact (to switch from CPU to GPU) and it might be due to the low parallelization of recurrent neural network. 

Switching from CPU to GPU when batch size = 160 -> speed up of 1.6 fold. 

Te number of speed up folds decrease when batch size is lower (lower parallelization); when batch size = 1, the speed up folds decreases to 1.10.

## Batch size's effect on model inference time (using CPU)
when batch size = 1, inference time = 0.12681238253911337
                     train time = unknown
when batch size = 160, inference time = 0.009   -> 50 folds fastser
                       train time = 0.5

## Pickle data saving/loading 
We test the time it takes for python to save dataframe, simple variables, and tensors. We did not record the data, but it shows the 10 fold speed up can be achieved if we just save the data once and load the data instead of processing them again.

This is very helpful in hyperparaemter tuning when we have to do the same data processing again and again.

We also test the strategy of save-then-load data on autoforecast-VA. We observe that this strategy could potentially let autoforecast-VA achieve about 5 folds speed up.

data preparation:  28.25, 34.77
train data processing duration 17.91, 21.38
test data processing duration 5.37, 5.56

save data prepartion:  1.706, 1.86
load data prepration 6.95, 7.88

save train data 1.01, 1.16
load train data 1.52, 1.53

save test data 0.24, 0.29
load test data 0.37, 0.45

## Caveats
Normal computation using tensor is very slow. Computation can become much faster when convert them back to numpy array. Of course, when training model, the data should be in tensors, but in other situations (doing reverse normalization for the predictions generated by the model), we should convert back to numpy array first.